---
title: "Analysis v1.2 mash"
author: "Alice MacQueen"
date: "2023-08-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(bigsnpr)
library(mashr)
library(switchgrassGWAS)
library(rlist)
library(cowplot)
library(here)
library(zoo)
library(rrBLUP)
```

# Installation reminders for any reinstalls
On debian: `apt install libgsl-dev` to install the GNU Scientific Library, zB.

For mashr: Sometimes install.packages("RcppGSL") is also needed.
#

Sam used a greedy mash algorithm to find the best set of covariance matrices to include, using 19K randomly associated variants.

Now use the best set of covariance matrices on 19K strongly associated variants.

Step 0. Set up.
  0a. Look at phenotype distribution & heritability; run GWAS to get univariate SNP effect estimates & standard errors. Already done.
  0b. Make hypothesis covariance matrices based on weather data from 2018 - 2020. For cumulative weather variables, use a range of days, 1 - 20, say. Come back and add more if 20 is chosen by greedy mash.
Step 1. Run greedy mash to find best set of covariance matrices that explain SNP effect variation across environments, using 19K 'randomly associated variants'. Go back to 0b. if high number of day covar matrices tend to be chosen by greedy mash.
Step 2. Use covariances from (1) to run mash on 19K 'strongly associated variants'. 
3. Look at loadings on covariance matrices and meanings of covariance matrices as the writeup (Figure 2, 3; Table 1). Look at amount of antagonistic pleiotropy vs same sign significant effects.
4. Repeat the QTL analysis... with Li.

How much of a lift is it to re-run the greedy mash algorithm? If we, or if reviewers are thinking about this in a Jiaming Yu sense, I think the next logical question to ask is 'Why these covariance matrices'? The overly honest answer for the current set is that there were computational limits to running mash with so many covariance matrices, so I chose a few arbitrary time points for each weather variable we thought might be a environmental driver. But now we have introduced a method to choose between highly correlated covariance matrices, perhaps we could do better. 
2. 


# ----
# Generate weather variables

# Covar 1-28d weather
As of 2023-09-11, want to remake these per reviewer revision to have h2 on the diagonal instead of the CV. Need to estimate h2 for these new covar matrices, probably using rrBLUP.


## load phe & wea data
```{r}
phe_ten_sites <- readRDS(here("data", "Phenotypes_2018_to_2021_10_sites.rds"))
sites <- readRDS("~/Github/pvdiv-phenology-gxe/data/sites.rds")
jday_phe <- readRDS("~/Github/pvdiv-phenology-gxe/data/Julian_date_GDD_and_rainfall_phenotypes_4wcr_and_gwas.rds")
wea_ten_sites <- readRDS(file = here("data", "Daily_weather_2018_to_2021_10_sites.rds"))

##function to calculate daylength

day.length.hrs <- function(day, latitude) {

  daylength.coeff <- asin(0.39795*cos(0.2163108 + 2*atan(0.9671396*tan(0.00860*(day - 186)))))

  daylength.hrs <- 24 - (24/pi)*acos((sin(0.8333*pi/180) + sin(latitude*pi/180)*sin(daylength.coeff))/(cos(latitude*pi/180)*cos(daylength.coeff)))

  return(daylength.hrs)

}

phe_2019 <- jday_phe %>%
  select(-(CGDD_12C:dyln_change_sec)) %>%
  pivot_longer(cols = GR50:FL50, names_to = "PHE", values_to = "JDAY")
```

```{r}

wea_ten_sites <- wea_ten_sites %>%
  mutate(JDAY = yday(Working_date),
         DYLN = day.length.hrs(day = JDAY, latitude = Latitude),
         tmax_sm = ifelse(is.na(tmax),
                          (na.locf(tmax) + na.locf(tmax, fromLast = TRUE))/2,
                          tmax),
         tmin_sm = ifelse(is.na(tmin),
                          (na.locf(tmin) + na.locf(tmin, fromLast = TRUE))/2,
                          tmin),
         tmean_sm = ifelse(is.na(Tmean),
                          (na.locf(Tmean) + na.locf(Tmean, fromLast = TRUE))/2,
                          Tmean),
         GDD_12Cbase = ifelse((tmax_sm + tmin_sm)/2 >= 12.0,
                              (tmax_sm + tmin_sm)/2 - 10,
                              0),
         Year = year(Working_date)) %>%
  mutate(JDAY_EXP = case_when(Year == 2018 ~ JDAY,
                              Year == 2019 ~ JDAY + 365,
                              Year == 2020 ~ JDAY + (365*2),
                              Year == 2021 ~ JDAY + (365*3),
                              TRUE ~ NA_real_)) %>%
  arrange(Latitude, Year, JDAY_EXP) %>%
  select(-JDAY) %>%
  group_by(manu_site) %>%
  mutate(crain_2d = prcp + lag(prcp, default = 0),
         crain_3d = prcp + lag(prcp, default = 0) + 
           lag(prcp, default = 0, n = 2),
         crain_4d = prcp + lag(prcp, default = 0) + 
           lag(prcp, default = 0, n = 2) + 
           lag(prcp, default = 0, n = 3),
         crain_5d = prcp + lag(prcp, default = 0) + 
           lag(prcp, default = 0, n = 2) + lag(prcp, default = 0, n = 3) +
           lag(prcp, default = 0, n = 4),
         crain_6d = prcp + lag(prcp, default = 0) + 
           lag(prcp, default = 0, n = 2) + lag(prcp, default = 0, n = 3) +
           lag(prcp, default = 0, n = 4) +
           lag(prcp, default = 0, n = 5),
         crain_7d = prcp + lag(prcp, default = 0) + 
           lag(prcp, default = 0, n = 2) + lag(prcp, default = 0, n = 3) +
           lag(prcp, default = 0, n = 4) +
           lag(prcp, default = 0, n = 5) +
           lag(prcp, default = 0, n = 6),
         dyln_change_sec = (DYLN - lag(DYLN))*60*60,
         dyln_change_sec_2d_prior = (lag(DYLN, n = 1) - 
                                       lag(DYLN, n = 2))*60*60,
         dyln_change_sec_3d_prior = (lag(DYLN, n = 2) - 
                                       lag(DYLN, n = 3))*60*60,
         dyln_change_sec_4d_prior = (lag(DYLN, n = 3) - 
                                       lag(DYLN, n = 4))*60*60,
         dyln_change_sec_5d_prior = (lag(DYLN, n = 4) - 
                                       lag(DYLN, n = 5))*60*60,
         dyln_change_sec_6d_prior = (lag(DYLN, n = 5) - 
                                       lag(DYLN, n = 6))*60*60,
         dyln_change_sec_7d_prior = (lag(DYLN, n = 6) - 
                                       lag(DYLN, n = 7))*60*60,
         dyln_change_sec_14d_prior = (lag(DYLN, n = 13) - 
                                       lag(DYLN, n = 14))*60*60,
         dyln_2d_prior = lag(DYLN, n = 1),
         dyln_3d_prior = lag(DYLN, n = 2),
         dyln_4d_prior = lag(DYLN, n = 3),
         dyln_5d_prior = lag(DYLN, n = 4),
         dyln_6d_prior = lag(DYLN, n = 5),
         dyln_7d_prior = lag(DYLN, n = 6),
         dyln_14d_prior = lag(DYLN, n = 13),
         cgdd_12c_2d = (GDD_12Cbase + lag(GDD_12Cbase, default = 0))/2,
         tave_2d = (tmean_sm + lag(tmean_sm, default = 0))/2,
         cgdd_12c_3d = (GDD_12Cbase + lag(GDD_12Cbase, default = 0) + 
           lag(GDD_12Cbase, default = 0, n = 2))/3,
         tave_3d = (tmean_sm + lag(tmean_sm, default = 0) + 
           lag(tmean_sm, default = 0, n = 2))/3,
         cgdd_12c_4d = (GDD_12Cbase + lag(GDD_12Cbase, default = 0) + 
           lag(GDD_12Cbase, default = 0, n = 2) + 
           lag(GDD_12Cbase, default = 0, n = 3))/4,
         tave_4d = (tmean_sm + lag(tmean_sm, default = 0) + 
           lag(tmean_sm, default = 0, n = 2) + 
           lag(tmean_sm, default = 0, n = 3))/4,
         cgdd_12c_5d = (GDD_12Cbase + lag(GDD_12Cbase, default = 0) + 
           lag(GDD_12Cbase, default = 0, n = 2) + 
           lag(GDD_12Cbase, default = 0, n = 3) +
           lag(GDD_12Cbase, default = 0, n = 4))/5,
         tave_5d = (tmean_sm + lag(tmean_sm, default = 0) + 
           lag(tmean_sm, default = 0, n = 2) + 
           lag(tmean_sm, default = 0, n = 3) +
           lag(tmean_sm, default = 0, n = 4))/5,
         cgdd_12c_6d = (GDD_12Cbase + lag(GDD_12Cbase, default = 0) + 
           lag(GDD_12Cbase, default = 0, n = 2) + 
           lag(GDD_12Cbase, default = 0, n = 3) +
           lag(GDD_12Cbase, default = 0, n = 4) +
           lag(GDD_12Cbase, default = 0, n = 5))/6,
         tave_6d = (tmean_sm + lag(tmean_sm, default = 0) + 
           lag(tmean_sm, default = 0, n = 2) + 
           lag(tmean_sm, default = 0, n = 3) +
           lag(tmean_sm, default = 0, n = 4) +
           lag(tmean_sm, default = 0, n = 5))/6,
         cgdd_12c_7d = (GDD_12Cbase + lag(GDD_12Cbase, default = 0) + 
           lag(GDD_12Cbase, default = 0, n = 2) + 
           lag(GDD_12Cbase, default = 0, n = 3) +
           lag(GDD_12Cbase, default = 0, n = 4) +
           lag(GDD_12Cbase, default = 0, n = 5) +
           lag(GDD_12Cbase, default = 0, n = 6))/7,
         tave_7d = (tmean_sm + lag(tmean_sm, default = 0) + 
           lag(tmean_sm, default = 0, n = 2) + 
           lag(tmean_sm, default = 0, n = 3) +
           lag(tmean_sm, default = 0, n = 4) +
           lag(tmean_sm, default = 0, n = 5) +
           lag(tmean_sm, default = 0, n = 6))/7,
         cgdd_12c_14d = (GDD_12Cbase + lag(GDD_12Cbase, default = 0) + 
           lag(GDD_12Cbase, default = 0, n = 2) + 
           lag(GDD_12Cbase, default = 0, n = 3) +
           lag(GDD_12Cbase, default = 0, n = 4) +
           lag(GDD_12Cbase, default = 0, n = 5) +
           lag(GDD_12Cbase, default = 0, n = 6) +
           lag(GDD_12Cbase, default = 0, n = 7) +
           lag(GDD_12Cbase, default = 0, n = 8) +
           lag(GDD_12Cbase, default = 0, n = 9) +
           lag(GDD_12Cbase, default = 0, n = 10) +
           lag(GDD_12Cbase, default = 0, n = 11) +
           lag(GDD_12Cbase, default = 0, n = 12) +
           lag(GDD_12Cbase, default = 0, n = 13))/14,
         tave_14d = (tmean_sm + lag(tmean_sm, default = 0) + 
           lag(tmean_sm, default = 0, n = 2) + 
           lag(tmean_sm, default = 0, n = 3) +
           lag(tmean_sm, default = 0, n = 4) +
           lag(tmean_sm, default = 0, n = 5) +
           lag(tmean_sm, default = 0, n = 6) +
           lag(tmean_sm, default = 0, n = 7) +
           lag(tmean_sm, default = 0, n = 8) +
           lag(tmean_sm, default = 0, n = 9) +
           lag(tmean_sm, default = 0, n = 10) +
           lag(tmean_sm, default = 0, n = 11) +
           lag(tmean_sm, default = 0, n = 12) +
           lag(tmean_sm, default = 0, n = 13))/14,
         cgdd_12c_21d = (GDD_12Cbase + lag(GDD_12Cbase, default = 0) + 
           lag(GDD_12Cbase, default = 0, n = 2) + 
           lag(GDD_12Cbase, default = 0, n = 3) +
           lag(GDD_12Cbase, default = 0, n = 4) +
           lag(GDD_12Cbase, default = 0, n = 5) +
           lag(GDD_12Cbase, default = 0, n = 6) +
           lag(GDD_12Cbase, default = 0, n = 7) +
           lag(GDD_12Cbase, default = 0, n = 8) +
           lag(GDD_12Cbase, default = 0, n = 9) +
           lag(GDD_12Cbase, default = 0, n = 10) +
           lag(GDD_12Cbase, default = 0, n = 11) +
           lag(GDD_12Cbase, default = 0, n = 12) +
           lag(GDD_12Cbase, default = 0, n = 13) +
           lag(GDD_12Cbase, default = 0, n = 14) +
           lag(GDD_12Cbase, default = 0, n = 15) +
           lag(GDD_12Cbase, default = 0, n = 16) +
           lag(GDD_12Cbase, default = 0, n = 17) +
           lag(GDD_12Cbase, default = 0, n = 18) +
           lag(GDD_12Cbase, default = 0, n = 19) +
           lag(GDD_12Cbase, default = 0, n = 20))/21,
         tave_21d = (tmean_sm + lag(tmean_sm, default = 0) + 
           lag(tmean_sm, default = 0, n = 2) + 
           lag(tmean_sm, default = 0, n = 3) +
           lag(tmean_sm, default = 0, n = 4) +
           lag(tmean_sm, default = 0, n = 5) +
           lag(tmean_sm, default = 0, n = 6) +
           lag(tmean_sm, default = 0, n = 7) +
           lag(tmean_sm, default = 0, n = 8) +
           lag(tmean_sm, default = 0, n = 9) +
           lag(tmean_sm, default = 0, n = 10) +
           lag(tmean_sm, default = 0, n = 11) +
           lag(tmean_sm, default = 0, n = 12) +
           lag(tmean_sm, default = 0, n = 13) +
           lag(tmean_sm, default = 0, n = 14) +
           lag(tmean_sm, default = 0, n = 15) +
           lag(tmean_sm, default = 0, n = 16) +
           lag(tmean_sm, default = 0, n = 17) +
           lag(tmean_sm, default = 0, n = 18) +
           lag(tmean_sm, default = 0, n = 19) +
           lag(tmean_sm, default = 0, n = 20))/21,
         cgdd_12c_28d = (GDD_12Cbase + lag(GDD_12Cbase, default = 0) + 
           lag(GDD_12Cbase, default = 0, n = 2) + 
           lag(GDD_12Cbase, default = 0, n = 3) +
           lag(GDD_12Cbase, default = 0, n = 4) +
           lag(GDD_12Cbase, default = 0, n = 5) +
           lag(GDD_12Cbase, default = 0, n = 6) +
           lag(GDD_12Cbase, default = 0, n = 7) +
           lag(GDD_12Cbase, default = 0, n = 8) +
           lag(GDD_12Cbase, default = 0, n = 9) +
           lag(GDD_12Cbase, default = 0, n = 10) +
           lag(GDD_12Cbase, default = 0, n = 11) +
           lag(GDD_12Cbase, default = 0, n = 12) +
           lag(GDD_12Cbase, default = 0, n = 13) +
           lag(GDD_12Cbase, default = 0, n = 14) +
           lag(GDD_12Cbase, default = 0, n = 15) +
           lag(GDD_12Cbase, default = 0, n = 16) +
           lag(GDD_12Cbase, default = 0, n = 17) +
           lag(GDD_12Cbase, default = 0, n = 18) +
           lag(GDD_12Cbase, default = 0, n = 19) +
           lag(GDD_12Cbase, default = 0, n = 20) +
           lag(GDD_12Cbase, default = 0, n = 21) +
           lag(GDD_12Cbase, default = 0, n = 22) +
           lag(GDD_12Cbase, default = 0, n = 23) +
           lag(GDD_12Cbase, default = 0, n = 24) +
           lag(GDD_12Cbase, default = 0, n = 25) +
           lag(GDD_12Cbase, default = 0, n = 26) +
           lag(GDD_12Cbase, default = 0, n = 27))/28,
         tave_28d = (tmean_sm + lag(tmean_sm, default = 0) + 
           lag(tmean_sm, default = 0, n = 2) + 
           lag(tmean_sm, default = 0, n = 3) +
           lag(tmean_sm, default = 0, n = 4) +
           lag(tmean_sm, default = 0, n = 5) +
           lag(tmean_sm, default = 0, n = 6) +
           lag(tmean_sm, default = 0, n = 7) +
           lag(tmean_sm, default = 0, n = 8) +
           lag(tmean_sm, default = 0, n = 9) +
           lag(tmean_sm, default = 0, n = 10) +
           lag(tmean_sm, default = 0, n = 11) +
           lag(tmean_sm, default = 0, n = 12) +
           lag(tmean_sm, default = 0, n = 13) +
           lag(tmean_sm, default = 0, n = 14) +
           lag(tmean_sm, default = 0, n = 15) +
           lag(tmean_sm, default = 0, n = 16) +
           lag(tmean_sm, default = 0, n = 17) +
           lag(tmean_sm, default = 0, n = 18) +
           lag(tmean_sm, default = 0, n = 19) +
           lag(tmean_sm, default = 0, n = 20) +
           lag(tmean_sm, default = 0, n = 21) +
           lag(tmean_sm, default = 0, n = 22) +
           lag(tmean_sm, default = 0, n = 23) +
           lag(tmean_sm, default = 0, n = 24) +
           lag(tmean_sm, default = 0, n = 25) +
           lag(tmean_sm, default = 0, n = 26) +
           lag(tmean_sm, default = 0, n = 27))/28,
         )

```

```{r}
wea_ten_sites
```

# Define phenotypes
```{r}
Tbase <- 12

phe_gr_fl <- phe_2019 %>%
  mutate(JDAY_EXP = case_when(YEAR == 2018 ~ JDAY,
                              YEAR == 2019 ~ JDAY + 365,
                              YEAR == 2020 ~ JDAY + (365*2),
                              YEAR == 2021 ~ JDAY + (365*3),
                              TRUE ~ NA_real_)) %>%
  select(-JDAY) %>%
  pivot_wider(names_from = PHE, values_from = JDAY_EXP) %>%
  left_join(select(sites, SITE, manu_site)) %>%
  mutate(Region = case_when(manu_site %in% c("SD", "MO", "IL", "MI", "NE") ~ "North",
                            manu_site %in% c("TX1", "TX2", "TX3", "TX4") ~ "Texas",
                                             TRUE ~ "OK"))

tmp2 <- wea_ten_sites %>%
  select(-Year)

gr_ten_sites <- phe_gr_fl %>%
  ungroup() %>%
  mutate(GR50 = floor(GR50)) %>%
  filter(!is.na(GR50)) %>%
  left_join(tmp2, by = c("manu_site", "GR50" = "JDAY_EXP")) %>%
  select(-SITE, -FL50, -(Working_date:wndk), -Tdrnl, -(Latitude:Elevation), -(tmax_sm:tmean_sm), -DLYN) %>%
  rename(crain_1d = prcp, dyln_gr50 = DYLN, cgdd_12c_1d = GDD_12Cbase, tave_1d = Tmean)

fl_ten_sites <- phe_gr_fl %>%
  ungroup() %>%
  filter(!is.na(FL50)) %>%
  mutate(FL50 = floor(FL50),
         GR50 = floor(GR50)) %>%
  left_join(tmp2, by = c("manu_site", "FL50" = "JDAY_EXP")) %>%
  select(-SITE, -(Working_date:wndk), -Tdrnl, -(Latitude:Elevation), -(tmax_sm:tmean_sm), -DLYN) %>%
  rename(crain_1d = prcp, dyln_gr50 = DYLN, cgdd_12c_1d = GDD_12Cbase, tave_1d = Tmean)
# 11 September flowering dates in 2021 at Pickle have NA's for weather values. Decided to let this be.

```

## save weather-based phenotypes
```{r}
gr_ten_sites %>%
  saveRDS(file = here("data", "Weather_related_phe_GR50_2019.rds"))
fl_ten_sites %>%
  saveRDS(file = here("data", "Weather_related_phe_FL50_2019.rds"))
```
# --- can start here
# Covar csv for greedy mash algorithm

Make & save hypothesis matrices in a hypothesis_matrices directory in data. 
Then run the greedy mash algorithm again.

phe_hyp <- c("cgdd_12c_10d", "cgdd_12c_18d", "cgdd_12c_5d", 
             "tave_5d", "tave_10d", "tave_18d")
phe_hyp <- c("FL50", "GR50", "dyln_fl50", "dyln_change_sec", "cgdd_12c_gr2fl", 
             "crain_gr2fl", "crain_1d", "crain_3d", "crain_5d")



### load weather phenotypes
```{r}
gr_ten_sites <- readRDS(file = here("data", 
                                    "Weather_related_phe_GR50_2019.rds"))
fl_ten_sites <- readRDS(file = here("data", 
                                    "Weather_related_phe_FL50_2019.rds"))
```

```{r}
phe_hyp <- colnames(fl_ten_sites)[c(5,9:51)]
colnames(gr_ten_sites)[8:50]
```

## Calculate heritability for diagonal
I don't have access to ASREML any more, so need a different package to do this.

### sketch out how h2 is calculated in rrBLUP

Use the kinship matrix I've already calculated (using the van Raden method, using switchgrassGWAS::pvdiv_kinship()) using the SNPs as the K matrix.
```{r}
library(rrBLUP)
k_full <- read_rds("~/Github/pvdiv-genome/tensite_twoyear/Kinship_van_Raden_630_individuals_SNPs_r2_20percent.rds")
fl_ten_sites
colnames(k_full)

phe_one <- fl_ten_sites %>% filter(manu_site == "TX1") %>%
  filter(WHERE != "FWCR") %>%
  group_by(PLANT_ID) %>%
  summarise(crain_1d = mean(crain_1d, na.rm = TRUE)) %>%
  filter(!is.na(PLANT_ID))
k_in_phe <- which(colnames(k_full) %in% phe_one$PLANT_ID)
k_one <- k_full[k_in_phe, k_in_phe]

nrow(k_one)
nrow(phe_one)
mod2 <- mixed.solve(y = phe_one$crain_1d, K = k_one)
h2 <- mod2$Vu/(mod2$Vu + mod2$Ve)
```

Another error: "Error in mash(data, covmats) : 
  All U_scaled matrices should be positive semi-definite

So I should adjust these to make sure all are positive semi-definite, too, in addition to replacing the diagonals with h2.

### Loop to calculate covar for flowering
```{r}
metadata <- read_rds(here("data", "metadata.rds"))
subpop_v2 <- list(all = c("Gulf", "Midwest"), midwest = "Midwest",
                  gulf = "Gulf")
year_v = c(2019, 2020, 2021)

fl_ten_sites <- fl_ten_sites %>%
  left_join(select(metadata, PLANT_ID, SUBPOP)) %>%
  rename(Year = YEAR)
fl_ten_sites$PLANT_ID <- factor(fl_ten_sites$PLANT_ID, levels = colnames(k_full))

# change based on phenotype ---------------
#phe_hyp <- c("FL50", "GR50", "dyln_fl50", "dyln_change_sec", 
#             "dyln_change_sec_7d_prior", "dyln_change_sec_14d_prior", 
#             "dyln_7d_prior", "dyln_14d_prior", "cgdd_12c_gr2fl", 
#             "crain_gr2fl", "crain_1d", "crain_3d", "crain_5d")

phe_hyp <- colnames(fl_ten_sites)[c(5,9:51)]
i=2
k=1

U_hyp_2019 <- list()
for(i in seq_along(phe_hyp)){
  for(k in seq_along(subpop_v2)){

    cov_df <- fl_ten_sites %>%
      ungroup() %>%
      filter(SUBPOP %in% subpop_v2[[k]]) %>%
      filter(Year %in% 2019) %>%
      select(PLANT_ID, SUBPOP, manu_site, phe_hyp[i]) %>%
      group_by(PLANT_ID, SUBPOP, manu_site) %>%
      mutate(IND = row_number()) %>%
      pivot_wider(names_from = manu_site, values_from = phe_hyp[i]) %>%
      mutate(PLANT_ID = as.factor(PLANT_ID),
             IND = as.factor(IND)) %>%
      select(PLANT_ID, SUBPOP, IND, MI, MO, NE, OK, SD, TX1, TX2, TX3) %>% #####  IL,
      replace_na(list(MI = 0, MO = 0, NE = 0, OK = 0, SD = 0, TX1 = 0, TX2 = 0, 
                      TX3 = 0))
    
    cor_phe <- cor(cov_df[,-(1:3)], use = "pairwise")
    # Set diagonal of this matrix = heritability within each garden
    h2_diag <- c()
    site_names <- c("MI", "MO", "NE", "OK", "SD", "TX1", "TX2", "TX3")
    for(j in seq_along(site_names)) {
      phe_one <- fl_ten_sites %>% filter(manu_site == site_names[j]) %>%
        filter(WHERE != "FWCR") %>%
        group_by(PLANT_ID) %>%
        summarise(phenotype = mean(!! sym(phe_hyp[i]), na.rm = TRUE)) %>%
        filter(!is.na(PLANT_ID))
      k_in_phe <- which(colnames(k_full) %in% phe_one$PLANT_ID)
      k_one <- k_full[k_in_phe, k_in_phe]
      mod2 <- mixed.solve(y = phe_one$phenotype, K = k_one)
      h2_diag[j] <- mod2$Vu/(mod2$Vu + mod2$Ve)
    }
    
    diag(cor_phe) <- h2_diag
    if (!matrixcalc::is.positive.semi.definite(cor_phe)) {
      cor_phe <- Matrix::nearPD(cor_phe)
    }
    U_hyp_2019 <- list.prepend(U_hyp_2019, cor_phe)
    names(U_hyp_2019)[1] <- paste0("flowering.", names(subpop_v2)[k], ".", 
                                   phe_hyp[i], ".U.mat")
  }
}

U_hyp_2019

# Test that every matrix is positive semi definite. Should all be true now changed code above.
U_psd <- c() 
for (i in seq_along(U_hyp_2019)) {
  if (is.matrix(U_hyp_2019[[i]])) {
    U_psd[i] <- matrixcalc::is.positive.semi.definite(U_hyp_2019[[i]])
  } else {
    U_psd[i] <- matrixcalc::is.positive.semi.definite(as.matrix(U_hyp_2019[[i]]$mat))
  }
  
}


for (i in seq_along(U_hyp_2019)) {
  if (is.matrix(U_hyp_2019[[i]])) {
    matrixout = matrix(unlist(U_hyp_2019[[i]]), ncol = 8, byrow = TRUE)
  } else {
    matrixout = matrix(unlist(U_hyp_2019[[i]]$mat), ncol = 8, byrow = TRUE)
  }
  row.names(matrixout) = site_names
  colnames(matrixout) = site_names
  #print(matrixout)
  write.csv(matrixout, file = here("analysis", "mash_greedy_algorithm", 
                                   "mash_h2_diag_covar", "matrices", 
                                   paste0(names(U_hyp_2019)[i], ".csv")))
}




```
Another error: "Error in mash(data, covmats) : 
  All U_scaled matrices should be positive semi-definite

### Loop to calculate covar for greenup
```{r}
gr_ten_sites <- gr_ten_sites %>%
  left_join(select(metadata, PLANT_ID, SUBPOP)) %>%
  rename(Year = YEAR)

# change based on phenotype ---------------
#phe_hyp <- c("cgdd_12c_10d", "cgdd_12c_20d", "cgdd_12c_5d", 
#             "tave_5d", "tave_10d", "tave_20d")
phe_hyp <- colnames(gr_ten_sites)[8:50]
i=1
k=1

U_hyp_2019 <- list()
for(i in seq_along(phe_hyp)){
  for(k in seq_along(subpop_v2)){

    cov_df <- gr_ten_sites %>%
      ungroup() %>%
      filter(SUBPOP %in% subpop_v2[[k]]) %>%
      filter(Year %in% 2019) %>%
      select(PLANT_ID, SUBPOP, manu_site, phe_hyp[i]) %>%
      group_by(PLANT_ID, SUBPOP, manu_site) %>%
      mutate(IND = row_number()) %>%
      pivot_wider(names_from = manu_site, values_from = phe_hyp[i]) %>%
      mutate(PLANT_ID = as.factor(PLANT_ID),
             IND = as.factor(IND)) %>%
      select(PLANT_ID, SUBPOP, IND, MI, MO, NE, OK, SD, TX1, TX2, TX3) #####  IL,
    
    cor_phe <- cor(cov_df[,-(1:3)], use = "pairwise")
    # Set diagonal of this matrix = heritability within each garden
    h2_diag <- c()
    site_names <- c("MI", "MO", "NE", "OK", "SD", "TX1", "TX2", "TX3")
    for(j in seq_along(site_names)) {
      phe_one <- fl_ten_sites %>% filter(manu_site == site_names[j]) %>%
        filter(WHERE != "FWCR") %>%
        group_by(PLANT_ID) %>%
        summarise(phenotype = mean(!! sym(phe_hyp[i]), na.rm = TRUE)) %>%
        filter(!is.na(PLANT_ID))
      k_in_phe <- which(colnames(k_full) %in% phe_one$PLANT_ID)
      k_one <- k_full[k_in_phe, k_in_phe]
      mod2 <- mixed.solve(y = phe_one$phenotype, K = k_one)
      h2_diag[j] <- mod2$Vu/(mod2$Vu + mod2$Ve)
    }
    
    diag(cor_phe) <- h2_diag
    if (!matrixcalc::is.positive.semi.definite(cor_phe)) {
      cor_phe <- Matrix::nearPD(cor_phe)
    }

    U_hyp_2019 <- list.prepend(U_hyp_2019, cor_phe)
    names(U_hyp_2019)[1] <- paste0("greenup.", names(subpop_v2)[k], ".", phe_hyp[i], ".U.mat")
  }
}

for (i in seq_along(U_hyp_2019)) {
  if (is.matrix(U_hyp_2019[[i]])) {
    matrixout = matrix(unlist(U_hyp_2019[[i]]), ncol = 8, byrow = TRUE)
  } else {
    matrixout = matrix(unlist(U_hyp_2019[[i]]$mat), ncol = 8, byrow = TRUE)
  }
  row.names(matrixout) = site_names
  colnames(matrixout) = site_names
  #print(matrixout)
  write.csv(matrixout, file = here("analysis", "mash_greedy_algorithm", 
                                   "mash_h2_diag_covar", "matrices", 
                                   paste0(names(U_hyp_2019)[i], ".csv")))
}



```

### Midwest covar fewer sites

#### flowering: 6 sites
MI, MO, SD, TX1, TX2, TX3
```{r flowering midwest}

phe_hyp <- colnames(fl_ten_sites)[c(5,9:51)]
i=2
k=2

U_hyp_2019 <- list()
for(i in seq_along(phe_hyp)){
  
    cov_df <- fl_ten_sites %>%
      ungroup() %>%
      filter(SUBPOP %in% subpop_v2[[k]]) %>%
      filter(Year %in% 2019) %>%
      select(PLANT_ID, SUBPOP, manu_site, phe_hyp[i]) %>%
      group_by(PLANT_ID, SUBPOP, manu_site) %>%
      mutate(IND = row_number()) %>%
      pivot_wider(names_from = manu_site, values_from = phe_hyp[i]) %>%
      mutate(PLANT_ID = as.factor(PLANT_ID),
             IND = as.factor(IND)) %>%
      select(PLANT_ID, SUBPOP, IND, MI, MO, SD, TX1, TX2, TX3) %>% #####  IL,
      replace_na(list(MI = 0, MO = 0, SD = 0, TX1 = 0, TX2 = 0, 
                      TX3 = 0))
    
    cor_phe <- cor(cov_df[,-(1:3)], use = "pairwise")
    # Set diagonal of this matrix = heritability within each garden
    h2_diag <- c()
    site_names <- c("MI", "MO", "SD", "TX1", "TX2", "TX3")
    for(j in seq_along(site_names)) {
      phe_one <- fl_ten_sites %>% filter(manu_site == site_names[j]) %>%
        filter(WHERE != "FWCR") %>%
        group_by(PLANT_ID) %>%
        summarise(phenotype = mean(!! sym(phe_hyp[i]), na.rm = TRUE)) %>%
        filter(!is.na(PLANT_ID))
      k_in_phe <- which(colnames(k_full) %in% phe_one$PLANT_ID)
      k_one <- k_full[k_in_phe, k_in_phe]
      mod2 <- mixed.solve(y = phe_one$phenotype, K = k_one)
      h2_diag[j] <- mod2$Vu/(mod2$Vu + mod2$Ve)
    }
    
    diag(cor_phe) <- h2_diag
    if (!matrixcalc::is.positive.semi.definite(cor_phe)) {
      cor_phe <- Matrix::nearPD(cor_phe)
    }
    U_hyp_2019 <- list.prepend(U_hyp_2019, cor_phe)
    names(U_hyp_2019)[1] <- paste0("flowering.", names(subpop_v2)[k], ".", 
                                   phe_hyp[i], ".U.mat")
}

U_hyp_2019

# Test that every matrix is positive semi definite. Should all be true now changed code above.
U_psd <- c() 
for (i in seq_along(U_hyp_2019)) {
  if (is.matrix(U_hyp_2019[[i]])) {
    U_psd[i] <- matrixcalc::is.positive.semi.definite(U_hyp_2019[[i]])
  } else {
    U_psd[i] <- matrixcalc::is.positive.semi.definite(as.matrix(U_hyp_2019[[i]]$mat))
  }
  
}


for (i in seq_along(U_hyp_2019)) {
  if (is.matrix(U_hyp_2019[[i]])) {
    matrixout = matrix(unlist(U_hyp_2019[[i]]), ncol = 6, byrow = TRUE)
  } else {
    matrixout = matrix(unlist(U_hyp_2019[[i]]$mat), ncol = 6, byrow = TRUE)
  }
  row.names(matrixout) = site_names
  colnames(matrixout) = site_names
  #print(matrixout)
  write.csv(matrixout, file = here("analysis", "mash_greedy_algorithm", 
                                   "mash_h2_diag_covar", "midwest_flowering", "matrices", 
                                   paste0(names(U_hyp_2019)[i], ".csv")))
}



```

```{r}
workingdir <- here("analysis", "mash_greedy_algorithm", 
                                   "mash_h2_diag_covar", "midwest_flowering")
pheno = "flowering"
betas = read.table(here(workingdir, "combined_mash_effects", paste0( pheno,'.midwest.betas.random.txt')))
stderrs = read.table(here(workingdir, "combined_mash_stderrs", paste0( pheno,'.midwest.stderrs.random.txt')))

mat_betas = matrix(unlist(betas),ncol=6,byrow = TRUE)
mat_stderrs = matrix(unlist(stderrs),ncol=6,byrow = TRUE)

sites = c('MI','MO','SD','TX1','TX2','TX3')
data=mash_set_data(mat_betas,mat_stderrs) 

canonical = cov_canonical(data = data)
matnames = ls(canonical)
for (matrix in matnames) {
  if (matrix %in% c('equal_effects','identity','simple_het_1','simple_het_2','simple_het_3')) {
  matrixout = matrix(unlist(canonical[matrix]), ncol = 6, byrow = TRUE)
  row.names(matrixout) = sites
  colnames(matrixout) = sites
  write.csv(matrixout, file = here(workingdir, "matrices", paste0(pheno,'.', matrix, '.csv')))
  }
}
print(canonical)
for (i in 1:6) {
  singleton = paste0('singletons_',i)
  print(singleton)
  singmat = canonical[singleton]
  site = sites[i]
  
  singmat = matrix(unlist(canonical[singleton]),ncol = 6, byrow = TRUE)
  row.names(singmat) = sites
  colnames(singmat) = sites
  write.csv(singmat, file = here(workingdir, "matrices", paste0(pheno, '.singleton.', site, '.csv')))
}

canonical = cov_pca(data,5)
cohort = "midwest"
effect = "random"
matnames = ls(canonical)
for (mat in matnames) {
  matrixout = matrix(unlist(canonical[mat]),ncol=6,byrow=TRUE)
  row.names(matrixout) = sites
  colnames(matrixout) = sites
  print(matrixout)
  write.csv(matrixout, file = here(workingdir, "matrices",
                                   paste0(pheno,'.', cohort,'.', effect,'.', mat, '.csv')))
}

```




#### greenup: 7 sites
MI, MO, NE, OK, TX1, TX2, TX3
```{r}
phe_hyp <- colnames(gr_ten_sites)[8:50]
i=1
k=2

U_hyp_2019 <- list()
for(i in seq_along(phe_hyp)){

    cov_df <- gr_ten_sites %>%
      ungroup() %>%
      filter(SUBPOP %in% subpop_v2[[k]]) %>%
      filter(Year %in% 2019) %>%
      select(PLANT_ID, SUBPOP, manu_site, phe_hyp[i]) %>%
      group_by(PLANT_ID, SUBPOP, manu_site) %>%
      mutate(IND = row_number()) %>%
      pivot_wider(names_from = manu_site, values_from = phe_hyp[i]) %>%
      mutate(PLANT_ID = as.factor(PLANT_ID),
             IND = as.factor(IND)) %>%
      select(PLANT_ID, SUBPOP, IND, MI, MO, NE, OK, TX1, TX2, TX3) #####  IL,
    
    cor_phe <- cor(cov_df[,-(1:3)], use = "pairwise")
    # Set diagonal of this matrix = heritability within each garden
    h2_diag <- c()
    site_names <- c("MI", "MO", "NE", "OK", "TX1", "TX2", "TX3")
    for(j in seq_along(site_names)) {
      phe_one <- fl_ten_sites %>% filter(manu_site == site_names[j]) %>%
        filter(WHERE != "FWCR") %>%
        group_by(PLANT_ID) %>%
        summarise(phenotype = mean(!! sym(phe_hyp[i]), na.rm = TRUE)) %>%
        filter(!is.na(PLANT_ID))
      k_in_phe <- which(colnames(k_full) %in% phe_one$PLANT_ID)
      k_one <- k_full[k_in_phe, k_in_phe]
      mod2 <- mixed.solve(y = phe_one$phenotype, K = k_one)
      h2_diag[j] <- mod2$Vu/(mod2$Vu + mod2$Ve)
    }
    
    diag(cor_phe) <- h2_diag
    if (!matrixcalc::is.positive.semi.definite(cor_phe)) {
      cor_phe <- Matrix::nearPD(cor_phe)
    }

    U_hyp_2019 <- list.prepend(U_hyp_2019, cor_phe)
    names(U_hyp_2019)[1] <- paste0("greenup.", names(subpop_v2)[k], ".", phe_hyp[i], ".U.mat")
}

for (i in seq_along(U_hyp_2019)) {
  if (is.matrix(U_hyp_2019[[i]])) {
    matrixout = matrix(unlist(U_hyp_2019[[i]]), ncol = 7, byrow = TRUE)
  } else {
    matrixout = matrix(unlist(U_hyp_2019[[i]]$mat), ncol = 7, byrow = TRUE)
  }
  row.names(matrixout) = site_names
  colnames(matrixout) = site_names
  #print(matrixout)
  write.csv(matrixout, file = here("analysis", "mash_greedy_algorithm", 
                                   "mash_h2_diag_covar", "midwest_greenup",
                                   "matrices", 
                                   paste0(names(U_hyp_2019)[i], ".csv")))
}


```

```{r}
workingdir <- here("analysis", "mash_greedy_algorithm", 
                                   "mash_h2_diag_covar", "midwest_greenup")
pheno = "greenup"
betas = read.table(here(workingdir, "combined_mash_effects", paste0( pheno,'.midwest.betas.random.txt')))
stderrs = read.table(here(workingdir, "combined_mash_stderrs", paste0( pheno,'.midwest.stderrs.random.txt')))

mat_betas = matrix(unlist(betas),ncol=7,byrow = TRUE)
mat_stderrs = matrix(unlist(stderrs),ncol=7,byrow = TRUE)

sites = c('MI','MO','NE', 'OK','TX1','TX2','TX3')
data=mash_set_data(mat_betas,mat_stderrs) 

canonical = cov_canonical(data = data)
matnames = ls(canonical)
for (matrix in matnames) {
  if (matrix %in% c('equal_effects','identity','simple_het_1','simple_het_2','simple_het_3')) {
  matrixout = matrix(unlist(canonical[matrix]), ncol = 7, byrow = TRUE)
  row.names(matrixout) = sites
  colnames(matrixout) = sites
  write.csv(matrixout, file = here(workingdir, "matrices", paste0(pheno,'.', matrix, '.csv')))
  }
}
print(canonical)
for (i in 1:7) {
  singleton = paste0('singletons_',i)
  print(singleton)
  singmat = canonical[singleton]
  site = sites[i]
  
  singmat = matrix(unlist(canonical[singleton]),ncol = 7, byrow = TRUE)
  row.names(singmat) = sites
  colnames(singmat) = sites
  write.csv(singmat, file = here(workingdir, "matrices", paste0(pheno, '.singleton.', site, '.csv')))
}

canonical = cov_pca(data,5)
cohort = "midwest"
effect = "random"
matnames = ls(canonical)
for (mat in matnames) {
  matrixout = matrix(unlist(canonical[mat]),ncol = 7,byrow=TRUE)
  row.names(matrixout) = sites
  colnames(matrixout) = sites
  print(matrixout)
  write.csv(matrixout, file = here(workingdir, "matrices",
                                   paste0(pheno,'.', cohort,'.', effect,'.', mat, '.csv')))
}

```

#### Visualize correlation matrices
```{r}

i=7
switchgrassGWAS::mash_plot_pairwise_sharing(corrmatrix = U_hyp_2019[[i]],
                                            reorder = FALSE)
switchgrassGWAS::mash_plot_pairwise_sharing(corrmatrix = U_hyp_2019[[i+1]],
                                            reorder = FALSE)
switchgrassGWAS::mash_plot_pairwise_sharing(corrmatrix = U_hyp_2019[[i+2]],
                                            reorder = FALSE) 
names(U_hyp_2019)[[i+2]]
```

# ---------------
# Info to run greedy mash algo

data/mash_switchgrass_models/mash/all_matrices/matrices hosts all covar mat (?)
analysis/mash_greedy_algorithm/mash_switchgrass_models/mash/all_matrices/matrices has more? covar mat?
Can cp from these matrices dir to mash_h2_diag_covar/matrices the following regex, which run.mash.greedy.search.py looks for:

 '.singleton.*'
'.identity.*'
'.simple_het_*.*'
 '.equal_effects.*'
'.*PCA*'


Set up new analysis/mash_greedy_algorithm/mash_1_to_28d_covar/ or subdirectory to have all the subdir necessary to run `run.mash.greedy.search.py` python script. I think these may have to be pre-generated for the script to work, and I'll make sure they're empty to keep there from being I/O issues I haven't tested for.

Make sure all matrices are present in matrices/ subdir and named & output appropriately for recognition

Make sure random and strong Bhat and Shat in combined_mash_effects and combined_mash_stderrs are appropriate - perhaps ultimately replace these with my inputs in mash/Subpop/inputs; perhaps use what's here as tests first

When I change to my inputs for effects & stderrs, I will need to adjust midwest covar & effects appropriately to reflect only 6 (flowering) and 7 (greenup) rows & columns used; the other GWAS were dropped as too low power or due to substantial phenotype data loss, & I think including them would be misleading, likely.

### Next steps
Redo Midwest: Remake Midwest covar matrices, including hypothesis, singleton, identity, simple het, equal effects, and data-driven matrices. Make mash_greedy.R variants for changed input dir & for different Midwest column numbers. 
Run two new run.mash.greedy.search.py commands:
python run.mash.greedy.search.py -p greenup -c midwest -e random
python run.mash.greedy.search.py -p flowering -c midwest -e random

Try Gulf & All with my known inputs:
python run.mash.greedy.search.py -p greenup -c gulf -e random
python run.mash.greedy.search.py -p greenup -c all -e random
python run.mash.greedy.search.py -p flowering -c gulf -e random
python run.mash.greedy.search.py -p flowering -c all -e random


Write inputs to mash_h2_diag_covar/All_inputs, Gulf_inputs, and Midwest_inputs, from the original mash work. Especially important for Midwest which has some poor GWAS at dropped sites.
The inputs Sam used have no column or rownames and look to be tab delimited.

aka combined_mash_effects/greenup.all.betas.random.txt 
and combined_mash_stderrs/flowering.gulf.betas.strong.txt

I only really need the random ones here, run mash again on the strong effects below.
```{r}
popkey = list(all = "Gulf_and_Midwest", midwest = "Midwest", gulf = "Gulf")
phekey = list(greenup = "GR50", flowering = "FL50")
# i=2; j=1

for (i in seq_along(popkey)) {
  for (j in seq_along(phekey)) {
    
    if (i == 2 & j == 1) {
      betas <- readRDS(file = here("analysis", "mash", popkey[[i]], "inputs", 
                                 paste0("B_hat_random_df_24000topSNPs_",
                                        phekey[[j]], "_", popkey[[i]], ".rds")))
      stderr <- readRDS(file = here("analysis", "mash", popkey[[i]], "inputs", 
                                 paste0("S_hat_random_df_24000topSNPs_",
                                        phekey[[j]], "_", popkey[[i]], ".rds")))
    } else if (i == 2 & j == 2) {
      betas <- readRDS(file = here("analysis", "mash", popkey[[i]], "inputs", 
                                 paste0("B_hat_random_df_33000topSNPs_",
                                        phekey[[j]], "_", popkey[[i]], ".rds")))
      stderr <- readRDS(file = here("analysis", "mash", popkey[[i]], "inputs", 
                                 paste0("S_hat_random_df_33000topSNPs_",
                                        phekey[[j]], "_", popkey[[i]], ".rds")))
    } else {
      betas <- readRDS(file = here("analysis", "mash", popkey[[i]], "inputs", 
                                 paste0("B_hat_random_df_19000topSNPs_",
                                        phekey[[j]], "_", popkey[[i]], ".rds")))
      stderr <- readRDS(file = here("analysis", "mash", popkey[[i]], "inputs", 
                                 paste0("S_hat_random_df_19000topSNPs_",
                                        phekey[[j]], "_", popkey[[i]], ".rds")))
    }
    write.table(betas, file = here("analysis", "mash_greedy_algorithm", 
                                   "mash_h2_diag_covar", 
                                   paste0(names(popkey)[i], "_inputs"), 
                                   paste0(names(phekey)[j], ".", 
                                          names(popkey)[i], 
                                          ".betas.random.txt")),
                row.names = FALSE, col.names = FALSE)
    write.table(stderr, file = here("analysis", "mash_greedy_algorithm", 
                                    "mash_h2_diag_covar", 
                                    paste0(names(popkey)[i], "_inputs"), 
                                    paste0(names(phekey)[j], ".", 
                                           names(popkey)[i], 
                                           ".stderrs.random.txt")),
                row.names = FALSE, col.names = FALSE)
  }
}

```

```{r}
for (i in 1:35) {
  if (!dir.exists(here("analysis", "mash_greedy_algorithm", "mash_h2_diag_covar", "midwest_inputs", "likelihoods", paste0("step", i)))) {
    dir.create(here("analysis", "mash_greedy_algorithm", "mash_h2_diag_covar", "midwest_inputs", "likelihoods", paste0("step", i)))
  }
}
```



# -----------------------------
# Run mash

```{r}
library(tidyverse)
library(bigsnpr)
library(switchgrassGWAS)
library(mashr)
library(rlist)
library(here)

source("~/Github/Functions_ggplot-theme-adjustments_2018-01-03.R")
mashdir <- here("analysis", "mash_greedy_algorithm")

#phe_gr_df <- read_rds(here("data",
#                           "Weather_related_phe_for_asreml_h2_GR50.rds"))
# all greenup related phenotypes, including greenup as functions of weather variables.
#phe_fl_df <- read_rds(here("data",
#                           "Weather_related_phe_for_asreml_h2_FL50.rds"))
# all flowering related phenotypes, including functions of weather variables.

# Files with the six suffixes I have/want mash results for:
numSNPs_v <- c(19000, 19000, 19000, 19000, 24000, 33000)
suffix_v <- c("FL50_Gulf_and_Midwest", "FL50_Gulf", "GR50_Gulf_and_Midwest",
              "GR50_Gulf", "GR50_Midwest", "FL50_Midwest")
suffix_vli <- c("Gulf_and_Midwest", "Gulf", "Gulf_and_Midwest",
              "Gulf", "Midwest", "Midwest")
subpop_v <- list(Gulf_and_Midwest = "363g_12.1M", Midwest = "134g_8.8M", 
                 Gulf = "229g_10.3M")

pop_sc <- c("all", "gulf", "all", "gulf", "midwest", "midwest")
phe_sc <- c("flowering", "flowering", "greenup", "greenup", "greenup",
            "flowering")

Upath <- here("analysis", "mash_greedy_algorithm", "mash_switchgrass_models", 
                "mash", "all_matrices", "matrices")
stopcondpath <- here("analysis", "mash_greedy_algorithm",
                     "mash_switchgrass_models", "mash", 
                     "all_matrices", "likelihood_paths")

for (i in 1:6) {  # change this for different scripts
  # B_hat_strong_df_19000topSNPs_FL50_Gulf_and_Midwest.rds
  # B_hat_strong_df_19000topSNPs_FL50_Gulf.rds
  # B_hat_strong_df_19000topSNPs_GR50_Gulf_and_Midwest.rds
  # B_hat_strong_df_19000topSNPs_GR50_Gulf.rds
  # B_hat_strong_df_24000topSNPs_GR50_Midwest.rds
  # B_hat_strong_df_33000topSNPs_FL50_Midwest.rds
  ## mash Gulf greenup
  numSNPs <- numSNPs_v[i]
  suffix <- suffix_v[i]
  # use this one for the likelihood paths
  # Read in likelihood paths, then use that to read in matrices, then use those to run mash.

  U_greedy <- 
    read_csv(file = file.path(stopcondpath, 
                              paste0(phe_sc[i], ".", pop_sc[i], 
                                     ".random.stop.condition.csv"))) %>%
    filter(matrix != "0.0")
  ## Read in Uhyp
  
  U_list <- list()
  for (k in 1:nrow(U_greedy)) {
    U_single <- read_csv(file = file.path(Upath, U_greedy$matrix[k]), show_col_types = FALSE)
    # if conditions for Midwest
    if (i == 5) {
    # MI, MO, NE, OK, TX1, TX2, TX3 used for GR50  
      U_single <- U_single %>%
        filter(`...1` %in% c("MI", "MO", "NE", "OK", "TX1", "TX2", "TX3")) %>%
        select(`...1`, .data$MI, .data$MO, .data$NE, .data$OK, .data$TX1,
               .data$TX2, .data$TX3)
    } else if (i == 6) {
    # MI, MO, SD, TX1, TX2, TX3 used for FL50  
      U_single <- U_single %>%
        filter(`...1` %in% c("MI", "MO", "SD", "TX1", "TX2", "TX3")) %>%
        select(`...1`, .data$MI, .data$MO, .data$SD, .data$TX1, .data$TX2,
               .data$TX3)
    }
    U_single <- U_single %>%
      select(-`...1`)
    U_single <- as.matrix(unname(U_single)) 
    
    
    # format cov matrices correctly for mash
    U_list <- list.append(U_list, U_single)
  }
  names(U_list) <- str_remove(U_greedy$matrix, ".csv")
  
  lipath <- here("analysis", "mash", suffix_vli[i], "inputs")
  
  list_input <- switchgrassGWAS:::load_mash_df(path = lipath, 
                                               numSNPs = numSNPs,
                                               suffix = suffix)
  # Just run mash itself as the greedy choice of cov matrices changes mash run
  # enough that mash_standard_run will not perform correctly (eg it always adds
  # all canonical covariance matrices).
  Bhat_strong <- as.matrix(list_input$B_hat_strong)
  Shat_strong <- as.matrix(list_input$S_hat_strong)
  Bhat_random <- as.matrix(list_input$B_hat_random)
  Shat_random <- as.matrix(list_input$S_hat_random)
  
  data_r <- mashr::mash_set_data(Bhat_random, Shat_random)
  Vhat <- mashr::estimate_null_correlation_simple(data = data_r)
  
  data_strong <- mashr::mash_set_data(Bhat_strong, Shat_strong, V = Vhat)
  data_random <- mashr::mash_set_data(Bhat_random, Shat_random, V = Vhat)
  
  # Run mash on the random dataset using the random data w/ correlation structure
  message(paste0("Fit mash to the random tests using both data-driven and ",
                 "canonical covariances."))
  # U_c <- mashr::cov_canonical(data_random)
  m = mashr::mash(data_random, Ulist = U_list, outputlevel = 1)
  
  # Run mash on the strong dataset (or all data) using
  # the previous results from the random data
  message(paste0("Compute posterior matrices for the strong effects",
                 " using the mash fit from the
                   random tests."))
  m2 = mashr::mash(data_strong, g = get_fitted_g(m), fixg = TRUE)
  
  saveRDS(m2, file = file.path(mashdir, suffix_vli[i], 
                                   paste0("Strong_mash_posterior_",
                                          "est_w_random_greedy_Ulist", 
                                          numSNPs, "_SNPs_",
                                          suffix, ".rds")))
  
  print(paste0(suffix, " mash outputs: "))
  print("Log likelihood with specified covariance matrices: ")
  print(get_loglik(m2), digits = 10)
  print("How many significant markers?")
  print(length(get_significant_results(m2)))
  switchgrassGWAS::mash_plot_covar(m2)
  switchgrassGWAS::mash_plot_manhattan_by_condition(m2)
  switchgrassGWAS::mash_plot_pairwise_sharing(m2)
  gxe <- switchgrassGWAS::get_GxE(m2)
  saveRDS(gxe, file = file.path(mashdir, suffix_vli[i], 
                                   paste0("GxE_default_",
                                          "est_w_random_greedy_Ulist",
                                          numSNPs, "_SNPs_",
                                          suffix, "_greedy_Ulist.rds")))
}
```

